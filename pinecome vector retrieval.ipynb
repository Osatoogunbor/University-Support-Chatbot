{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20fb1b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Pinecone index: ai-powered-chatbot\n",
      "✅ Pinecone index connected successfully!\n",
      "\n",
      "\n",
      "🔵 Welcome to the University Student Support Chatbot!\n",
      "🔹 Type your questions below. Type 'exit' or 'quit' to end.\n",
      "\n",
      "\n",
      "🟢 Your message: hello\n",
      "\n",
      "🔵 Chatbot: Hello! How can I assist you today?\n",
      "\n",
      "🟢 Your message: i need advice on assessment and deadlines\n",
      "✅ Retrieved 4 chunks from Pinecone after reranking.\n",
      "\n",
      "🔵 Chatbot: If your assessment deadline has passed but you haven't received a decision yet, don't worry. It can take up to 5 working days for your claim to be assessed. You should not submit your assessment during this time as it implies you are fit to sit and this invalidates the claim. There's no penalty for receiving a decision after the deadline, provided you submitted before the deadline. Also, remember that your assessment deadlines may also display tasks you've entered into your Office 365 calendar.\n",
      "\n",
      "🟢 Your message: i need advice on getting into msc artificial intelligence\n",
      "✅ Retrieved 2 chunks from Pinecone after reranking.\n",
      "\n",
      "🔵 Chatbot: To get into the MSc Artificial Intelligence course at the University of Wolverhampton, you don't need extensive maths or programming experience. The university offers a free online \"bridging course\" to help you improve your maths and programming skills. Additionally, the university provides online support sessions before you start the course to help you prepare. There are also opportunities to apply for paid, industry-supported internships.\n",
      "\n",
      "🟢 Your message: thank you\n",
      "✅ Retrieved 3 chunks from Pinecone after reranking.\n",
      "\n",
      "🔵 Chatbot: You're welcome! If you have any other questions or need further assistance, feel free to ask.\n",
      "\n",
      "🟢 Your message: exit\n",
      "🔴 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Allow nested event loops in Jupyter\n",
    "\n",
    "import os\n",
    "import tiktoken\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from transformers import pipeline\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. LOAD ENVIRONMENT & INITIALIZE\n",
    "# --------------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")  # e.g. \"us-east-1\"\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"❌ Missing OPENAI_API_KEY in .env file.\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"❌ Missing PINECONE_API_KEY in .env file.\")\n",
    "if not PINECONE_ENV:\n",
    "    raise ValueError(\"❌ Missing PINECONE_ENV in .env file.\")\n",
    "\n",
    "# Instantiate OpenAI and Pinecone clients\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. CONNECT OR CREATE THE PINECONE INDEX\n",
    "# --------------------------------------------------------------------------\n",
    "INDEX_NAME = \"ai-powered-chatbot\"\n",
    "index = pc.Index(INDEX_NAME)  # Directly connect to the existing Pinecone index\n",
    "print(\"✅ Connected to Pinecone index:\", INDEX_NAME)\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(\"✅ Pinecone index connected successfully!\\n\")\n",
    "\n",
    "\n",
    "# ✅ Load Sentiment Analysis Model\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    revision=\"714eb0f\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. DETECT GENERIC GREETINGS (INTENTS)\n",
    "# --------------------------------------------------------------------------\n",
    "GENERIC_INTENTS = {\n",
    "    \"hello\": \"Hello! How can I assist you today?\",\n",
    "    \"hi\": \"Hi! How can I help?\",\n",
    "    \"hey\": \"Hey! What do you need help with?\",\n",
    "    \"good morning\": \"Good morning! How can I assist?\",\n",
    "    \"bye\": \"Goodbye! Have a great day!\",\n",
    "    \"exit\": \"Goodbye! Take care!\",\n",
    "    \"quit\": \"Goodbye! See you next time!\"\n",
    "}\n",
    "\n",
    "def detect_generic_intent(query: str) -> str | None:\n",
    "    return GENERIC_INTENTS.get(query.strip().lower())\n",
    "\n",
    "# ✅ Function to Detect Sentiment\n",
    "def detect_sentiment(query):\n",
    "    result = sentiment_analyzer(query)[0]\n",
    "    return result['label'].lower()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. IMPROVED RETRIEVAL PROCESS WITH RERANKING\n",
    "# --------------------------------------------------------------------------\n",
    "async def retrieve_chunks(query: str, top_k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    1. Embed the user's query.\n",
    "    2. Query Pinecone for top_k matches.\n",
    "    3. Extract relevant text and emergency status.\n",
    "    4. Apply reranking for better retrieval.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Embed the query\n",
    "        embedding_response = await client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=query\n",
    "        )\n",
    "        query_vector = embedding_response.data[0].embedding\n",
    "\n",
    "        # Query Pinecone\n",
    "        result = index.query(vector=query_vector, top_k=top_k, include_metadata=True)\n",
    "\n",
    "        if not result.matches:\n",
    "            print(\"⚠️ No relevant matches found in Pinecone.\")\n",
    "            return []\n",
    "\n",
    "        retrieved_chunks = []\n",
    "        for match in result.matches:\n",
    "            meta = match.metadata or {}\n",
    "            chunk_text = meta.get(\"text_chunk\", \"\").strip()\n",
    "            category = meta.get(\"category\", \"unknown\").strip()\n",
    "            source = meta.get(\"source\", \"unknown\")\n",
    "            is_emergency = meta.get(\"is_emergency\", False)\n",
    "\n",
    "            if chunk_text:\n",
    "                retrieved_chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"category\": category,\n",
    "                    \"source\": source,\n",
    "                    \"is_emergency\": is_emergency,\n",
    "                    \"score\": match.score  # Use similarity score for reranking\n",
    "                })\n",
    "\n",
    "        # Prioritize emergency responses if found\n",
    "        emergency_chunks = [c for c in retrieved_chunks if c[\"is_emergency\"]]\n",
    "        if emergency_chunks:\n",
    "            print(\"⚠️ Emergency-related query detected! Prioritizing emergency responses.\")\n",
    "            return sorted(emergency_chunks, key=lambda x: -x[\"score\"])\n",
    "\n",
    "        # Reranking: Sort by score and source balance\n",
    "        sorted_chunks = sorted(retrieved_chunks, key=lambda x: -x[\"score\"])\n",
    "        print(f\"✅ Retrieved {len(sorted_chunks)} chunks from Pinecone after reranking.\")\n",
    "        return sorted_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Retrieval Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5. GENERATE A CHATBOT RESPONSE WITH IMPROVED CONTEXT\n",
    "# --------------------------------------------------------------------------\n",
    "async def generate_response(user_query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    - Checks for generic greetings first.\n",
    "    - Retrieves and reranks context.\n",
    "    - Uses improved GPT prompting.\n",
    "    \"\"\"\n",
    "\n",
    "    # A. Handle generic greetings\n",
    "    greeting_reply = detect_generic_intent(user_query)\n",
    "    if greeting_reply:\n",
    "        return greeting_reply\n",
    "\n",
    "    # B. Retrieve and rerank context\n",
    "    context_chunks = await retrieve_chunks(user_query, top_k=top_k)\n",
    "    if not context_chunks:\n",
    "        return \"I couldn't find relevant info. Could you try rephrasing your question?\"\n",
    "\n",
    "    # Remove duplicate or overly generic responses\n",
    "    unique_chunks = list({c[\"text\"]: c for c in context_chunks}.values())\n",
    "\n",
    "    # Combine for GPT prompt\n",
    "    combined_context = \"\\n\\n---\\n\\n\".join([c[\"text\"] for c in unique_chunks])\n",
    "\n",
    "    # C. Construct a more effective prompt for GPT\n",
    "    system_message = (\n",
    "        \"You are a University Student Support Chatbot. Use the retrieved information below \"\n",
    "        \"to answer the question accurately and concisely. Do NOT make up answers. \"\n",
    "        \"Prioritize emergency responses when necessary. Avoid repetition.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"User's question:\\n\\n{user_query}\\n\\n\"\n",
    "        f\"Relevant context:\\n\\n{combined_context}\\n\\n\"\n",
    "        \"Please provide a clear, concise, and relevant response.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        chat_response = await client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.8,  # Slightly higher temperature to encourage more natural responses\n",
    "            top_p=0.5  # Adjust top-p to allow for slight randomness and avoid repetitive responses\n",
    "        )\n",
    "        final_answer = chat_response.choices[0].message.content.strip()\n",
    "        return final_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating GPT response: {e}\")\n",
    "        return \"Oops, something went wrong.\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 6. MAIN CHATBOT LOOP\n",
    "# --------------------------------------------------------------------------\n",
    "async def main_chat_loop():\n",
    "    print(\"\\n🔵 Welcome to the University Student Support Chatbot!\")\n",
    "    print(\"🔹 Type your questions below. Type 'exit' or 'quit' to end.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\n🟢 Your message: \").strip()\n",
    "        if not user_input:\n",
    "            print(\"⚠️ No input detected. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"🔴 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Generate an answer\n",
    "        answer = await generate_response(user_input, top_k=5)\n",
    "        print(f\"\\n🔵 Chatbot: {answer}\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 7. RUN (ASYNC)\n",
    "# --------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main_chat_loop())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d80d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
