{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20fb1b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Pinecone index: ai-powered-chatbot\n",
      "‚úÖ Pinecone index connected successfully!\n",
      "\n",
      "\n",
      "üîµ Welcome to the University Student Support Chatbot!\n",
      "üîπ Type your questions below. Type 'exit' or 'quit' to end.\n",
      "\n",
      "\n",
      "üü¢ Your message: hello\n",
      "\n",
      "üîµ Chatbot: Hello! How can I assist you today?\n",
      "\n",
      "üü¢ Your message: what type of support does wlv student life connect give\n",
      "‚úÖ Retrieved 5 chunks from Pinecone after reranking.\n",
      "\n",
      "üîµ Chatbot: WLV Student Life Connect offers a variety of support services to students. These include advice, guidance, and support on mental health concerns, benefits and entitlements, relationships and childcare, debt management and finances, and legal advice. They also help with unresolved issues like parking tickets. The service is free, confidential, independent, and impartial, and is available to undergraduate, postgraduate, and apprentice learners. You can access this service via telephone at 0333 212 0672 or online at www.carefirst-lifestyle.co.uk. They also provide self-help resources such as leaflets and apps.\n",
      "\n",
      "üü¢ Your message: what are the weekly mental health workshops\n",
      "‚úÖ Retrieved 4 chunks from Pinecone after reranking.\n",
      "\n",
      "üîµ Chatbot: The weekly mental health workshops are online sessions held every Tuesday from 12 - 1 PM, led by mental health professionals from the university's Mental Health and Wellbeing Team. They cover various topics such as anxiety management, understanding the impact of anxiety on the body, managing negative thoughts, handling low mood, and strategies to break the cycle of procrastination. These workshops are open to all students and aim to share theory and techniques for managing common mental health difficulties. You can access these workshops via the \"Join the meeting now\" link.\n",
      "\n",
      "üü¢ Your message: what topics are covered in the workshop\n",
      "‚úÖ Retrieved 4 chunks from Pinecone after reranking.\n",
      "\n",
      "üîµ Chatbot: The workshops cover various topics related to mental health and wellbeing. Here are the topics:\n",
      "\n",
      "1. Anxiety: Bitesize - Introduction to anxiety management techniques and theory.\n",
      "2. Anxiety: Body and Mind - Understanding how anxiety impacts the body and how to soothe it.\n",
      "3. Anxiety: Automatic Negative Thoughts - Learning how thoughts influence anxiety and how to manage negative thinking.\n",
      "4. Managing Low Mood - Understanding why we experience low mood and how to manage it.\n",
      "5. Procrastination - Understanding the cycle of procrastination and strategies to break it.\n",
      "\n",
      "These workshops are held online every Tuesday from 12 - 1 PM. If you have suggestions for new workshops or need alternative times, you can email mhwenquiries@wlv.ac.uk.\n",
      "\n",
      "üü¢ Your message: exit\n",
      "üî¥ Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Allow nested event loops in Jupyter\n",
    "\n",
    "import os\n",
    "import tiktoken\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from transformers import pipeline\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. LOAD ENVIRONMENT & INITIALIZE\n",
    "# --------------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")  # e.g. \"us-east-1\"\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"‚ùå Missing OPENAI_API_KEY in .env file.\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"‚ùå Missing PINECONE_API_KEY in .env file.\")\n",
    "if not PINECONE_ENV:\n",
    "    raise ValueError(\"‚ùå Missing PINECONE_ENV in .env file.\")\n",
    "\n",
    "# Instantiate OpenAI and Pinecone clients\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. CONNECT OR CREATE THE PINECONE INDEX\n",
    "# --------------------------------------------------------------------------\n",
    "INDEX_NAME = \"ai-powered-chatbot\"\n",
    "index = pc.Index(INDEX_NAME)  # Directly connect to the existing Pinecone index\n",
    "print(\"‚úÖ Connected to Pinecone index:\", INDEX_NAME)\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(\"‚úÖ Pinecone index connected successfully!\\n\")\n",
    "\n",
    "\n",
    "# ‚úÖ Load Sentiment Analysis Model\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    revision=\"714eb0f\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. DETECT GENERIC GREETINGS (INTENTS)\n",
    "# --------------------------------------------------------------------------\n",
    "GENERIC_INTENTS = {\n",
    "    \"hello\": \"Hello! How can I assist you today?\",\n",
    "    \"hi\": \"Hi! How can I help?\",\n",
    "    \"hey\": \"Hey! What do you need help with?\",\n",
    "    \"good morning\": \"Good morning! How can I assist?\",\n",
    "    \"bye\": \"Goodbye! Have a great day!\",\n",
    "    \"exit\": \"Goodbye! Take care!\",\n",
    "    \"quit\": \"Goodbye! See you next time!\"\n",
    "}\n",
    "\n",
    "def detect_generic_intent(query: str) -> str | None:\n",
    "    return GENERIC_INTENTS.get(query.strip().lower())\n",
    "\n",
    "# ‚úÖ Function to Detect Sentiment\n",
    "def detect_sentiment(query):\n",
    "    result = sentiment_analyzer(query)[0]\n",
    "    return result['label'].lower()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. IMPROVED RETRIEVAL PROCESS WITH RERANKING\n",
    "# --------------------------------------------------------------------------\n",
    "async def retrieve_chunks(query: str, top_k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    1. Embed the user's query.\n",
    "    2. Query Pinecone for top_k matches.\n",
    "    3. Extract relevant text and emergency status.\n",
    "    4. Apply reranking for better retrieval.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Embed the query\n",
    "        embedding_response = await client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=query\n",
    "        )\n",
    "        query_vector = embedding_response.data[0].embedding\n",
    "\n",
    "        # Query Pinecone\n",
    "        result = index.query(vector=query_vector, top_k=top_k, include_metadata=True)\n",
    "\n",
    "        if not result.matches:\n",
    "            print(\"‚ö†Ô∏è No relevant matches found in Pinecone.\")\n",
    "            return []\n",
    "\n",
    "        retrieved_chunks = []\n",
    "        for match in result.matches:\n",
    "            meta = match.metadata or {}\n",
    "            chunk_text = meta.get(\"text_chunk\", \"\").strip()\n",
    "            category = meta.get(\"category\", \"unknown\").strip()\n",
    "            source = meta.get(\"source\", \"unknown\")\n",
    "            is_emergency = meta.get(\"is_emergency\", False)\n",
    "\n",
    "            if chunk_text:\n",
    "                retrieved_chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"category\": category,\n",
    "                    \"source\": source,\n",
    "                    \"is_emergency\": is_emergency,\n",
    "                    \"score\": match.score  # Use similarity score for reranking\n",
    "                })\n",
    "\n",
    "        # Prioritize emergency responses if found\n",
    "        emergency_chunks = [c for c in retrieved_chunks if c[\"is_emergency\"]]\n",
    "        if emergency_chunks:\n",
    "            print(\"‚ö†Ô∏è Emergency-related query detected! Prioritizing emergency responses.\")\n",
    "            return sorted(emergency_chunks, key=lambda x: -x[\"score\"])\n",
    "\n",
    "        # Reranking: Sort by score and source balance\n",
    "        sorted_chunks = sorted(retrieved_chunks, key=lambda x: -x[\"score\"])\n",
    "        print(f\"‚úÖ Retrieved {len(sorted_chunks)} chunks from Pinecone after reranking.\")\n",
    "        return sorted_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Retrieval Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5. GENERATE A CHATBOT RESPONSE WITH IMPROVED CONTEXT\n",
    "# --------------------------------------------------------------------------\n",
    "async def generate_response(user_query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    - Checks for generic greetings first.\n",
    "    - Retrieves and reranks context.\n",
    "    - Uses improved GPT prompting.\n",
    "    \"\"\"\n",
    "\n",
    "    # A. Handle generic greetings\n",
    "    greeting_reply = detect_generic_intent(user_query)\n",
    "    if greeting_reply:\n",
    "        return greeting_reply\n",
    "\n",
    "    # B. Retrieve and rerank context\n",
    "    context_chunks = await retrieve_chunks(user_query, top_k=top_k)\n",
    "    if not context_chunks:\n",
    "        return \"I couldn't find relevant info. Could you try rephrasing your question?\"\n",
    "\n",
    "    # Remove duplicate or overly generic responses\n",
    "    unique_chunks = list({c[\"text\"]: c for c in context_chunks}.values())\n",
    "\n",
    "    # Combine for GPT prompt\n",
    "    combined_context = \"\\n\\n---\\n\\n\".join([c[\"text\"] for c in unique_chunks])\n",
    "\n",
    "    # C. Construct a more effective prompt for GPT\n",
    "    system_message = (\n",
    "        \"You are a University Student Support Chatbot. Use the retrieved information below \"\n",
    "        \"to answer the question accurately and concisely. Do NOT make up answers. \"\n",
    "        \"Prioritize emergency responses when necessary. Avoid repetition.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"User's question:\\n\\n{user_query}\\n\\n\"\n",
    "        f\"Relevant context:\\n\\n{combined_context}\\n\\n\"\n",
    "        \"Please provide a clear, concise, and relevant response.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        chat_response = await client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.8,  # Slightly higher temperature to encourage more natural responses\n",
    "            top_p=0.5  # Adjust top-p to allow for slight randomness and avoid repetitive responses\n",
    "        )\n",
    "        final_answer = chat_response.choices[0].message.content.strip()\n",
    "        return final_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating GPT response: {e}\")\n",
    "        return \"Oops, something went wrong.\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 6. MAIN CHATBOT LOOP\n",
    "# --------------------------------------------------------------------------\n",
    "async def main_chat_loop():\n",
    "    print(\"\\nüîµ Welcome to the University Student Support Chatbot!\")\n",
    "    print(\"üîπ Type your questions below. Type 'exit' or 'quit' to end.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nüü¢ Your message: \").strip()\n",
    "        if not user_input:\n",
    "            print(\"‚ö†Ô∏è No input detected. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"üî¥ Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Generate an answer\n",
    "        answer = await generate_response(user_input, top_k=5)\n",
    "        print(f\"\\nüîµ Chatbot: {answer}\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 7. RUN (ASYNC)\n",
    "# --------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main_chat_loop())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d80d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
