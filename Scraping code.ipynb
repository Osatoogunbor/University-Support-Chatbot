{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e428020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://www.wlv.ac.uk/university-life/student-life/...\n",
      "Scraping https://www.wlv.ac.uk/current-students/student-support/student-support-and-wellbeing-ssw/advice-for-students-with-disabilities-and-specific-learning-disabilities/i-am-a-current-student/...\n",
      "Scraping https://www.wlv.ac.uk/current-students/student-support/mental-health-and-wellbeing-advice/...\n",
      "Scraping https://www.wlv.ac.uk/current-students/student-support/support-to-study-/...\n",
      "Scraping https://www.wlv.ac.uk/current-students/student-support/mental-health-and-wellbeing-advice/i-need-help-now/...\n",
      "Scraped data saved to scraped_data.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.wlv.ac.uk/university-life/student-life/\",\n",
    "    \"https://www.wlv.ac.uk/current-students/student-support/student-support-and-wellbeing-ssw/advice-for-students-with-disabilities-and-specific-learning-disabilities/i-am-a-current-student/\",\n",
    "    \"https://www.wlv.ac.uk/current-students/student-support/mental-health-and-wellbeing-advice/\",\n",
    "    \"https://www.wlv.ac.uk/current-students/student-support/support-to-study-/\",\n",
    "    \"https://www.wlv.ac.uk/current-students/student-support/mental-health-and-wellbeing-advice/i-need-help-now/\",\n",
    "]\n",
    "\n",
    "# Function to scrape a single URL\n",
    "def scrape_url(url, session):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = session.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        elements = soup.find_all(['h2', 'p'])\n",
    "        qa_pairs = []\n",
    "        current_heading = None\n",
    "        current_paragraphs = []\n",
    "        for element in elements:\n",
    "            if element.name == 'h2':\n",
    "                if current_heading and current_paragraphs:\n",
    "                    qa_pairs.append({\n",
    "                        \"question\": current_heading,\n",
    "                        \"answer\": \" \".join(current_paragraphs).strip()\n",
    "                    })\n",
    "                current_heading = element.text.strip()\n",
    "                current_paragraphs = []\n",
    "            elif element.name == 'p' and current_heading:\n",
    "                current_paragraphs.append(element.text.strip())\n",
    "        if current_heading and current_paragraphs:\n",
    "            qa_pairs.append({\n",
    "                \"question\": current_heading,\n",
    "                \"answer\": \" \".join(current_paragraphs).strip()\n",
    "            })\n",
    "        return qa_pairs\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Load existing scraped data (if any)\n",
    "try:\n",
    "    with open(\"scraped_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        all_scraped_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    all_scraped_data = []\n",
    "\n",
    "# Create a session\n",
    "session = requests.Session()\n",
    "\n",
    "# Scrape each URL and append the results\n",
    "for url in urls:\n",
    "    print(f\"Scraping {url}...\")\n",
    "    scraped_data = scrape_url(url, session)\n",
    "    all_scraped_data.extend(scraped_data)\n",
    "\n",
    "# Filter out irrelevant pairs\n",
    "irrelevant_keywords = [\"Read More\", \"WLV News\", \"Click here\", \"Read more\"]\n",
    "filtered_pairs = []\n",
    "for pair in all_scraped_data:\n",
    "    if (pair['answer'] and  # Ensure the answer is not empty\n",
    "        len(pair['answer'].split()) > 5 and  # Ensure the answer has at least 5 words\n",
    "        not any(keyword in pair['answer'] for keyword in irrelevant_keywords)):  # Exclude irrelevant pairs\n",
    "        filtered_pairs.append(pair)\n",
    "\n",
    "# Remove duplicate Q&A pairs\n",
    "unique_pairs = []\n",
    "seen = set()\n",
    "for pair in filtered_pairs:\n",
    "    pair_key = (pair['question'], pair['answer'])\n",
    "    if pair_key not in seen:\n",
    "        unique_pairs.append(pair)\n",
    "        seen.add(pair_key)\n",
    "\n",
    "# Save unique pairs to a JSON file\n",
    "with open(\"scraped_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(unique_pairs, file, indent=4)\n",
    "\n",
    "print(\"Scraped data saved to scraped_data.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb4959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
